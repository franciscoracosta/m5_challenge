{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import holidays\n",
    "from datetime import date\n",
    "\n",
    "import pywt\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAux functions\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aux functions\n",
    "\"\"\"\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nReading dataframes\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Sell prices has 6841121 rows and 4 columns\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "Calendar has 1969 rows and 14 columns\n",
      "Sales train validation has 30490 rows and 1947 columns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reading dataframes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "INPUT_DIR_PATH = ''\n",
    "DAYS_PRED = 28\n",
    "DATASET_SIZE = 1947\n",
    "TR_LAST = DATASET_SIZE - 28 - 28\n",
    "VL_LAST = DATASET_SIZE - 28\n",
    "TS_LAST = DATASET_SIZE\n",
    "\n",
    "def read_data():\n",
    "    sell_prices_df = pd.read_csv(INPUT_DIR_PATH + 'sell_prices.csv')\n",
    "    sell_prices_df = reduce_mem_usage(sell_prices_df)\n",
    "    print('Sell prices has {} rows and {} columns'.format(sell_prices_df.shape[0], sell_prices_df.shape[1]))\n",
    "\n",
    "    calendar_df = pd.read_csv(INPUT_DIR_PATH + 'calendar.csv')\n",
    "    calendar_df = reduce_mem_usage(calendar_df)\n",
    "    calendar_df = calendar_df.fillna('unknown')\n",
    "    print('Calendar has {} rows and {} columns'.format(calendar_df.shape[0], calendar_df.shape[1]))\n",
    "\n",
    "    sales_df = pd.read_csv(INPUT_DIR_PATH + 'sales_train_evaluation.csv')\n",
    "    print('Sales train validation has {} rows and {} columns'.format(sales_df.shape[0], sales_df.shape[1]))\n",
    "\n",
    "    submission_df = pd.read_csv(INPUT_DIR_PATH + 'sample_submission.csv')\n",
    "    return sell_prices_df, calendar_df, sales_df, submission_df\n",
    "    \n",
    "prices_df, calendar_df, sales_df, submission_df = read_data()\n",
    "\n",
    "num_cols = [f\"d_{day}\" for day in range(0,TR_LAST+1)]\n",
    "cat_cols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "\n",
    "df = pd.melt(sales_df,\n",
    "                  id_vars = cat_cols,\n",
    "                  value_vars = [col for col in sales_df.columns if col.startswith(\"d_\")],\n",
    "                  var_name = \"d\",\n",
    "                  value_name = \"sales\")\n",
    "\n",
    "df = df.merge(calendar_df, on= \"d\", copy = False)\n",
    "df = df.merge(prices_df, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "del sales_df, calendar_df, prices_df\n",
    "# gc.collect()\n",
    "\n",
    "df_raw = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EDA - https://www.kaggle.com/headsortails/back-to-predict-the-future-interactive-m5-eda\n",
    "\"\"\"\n",
    "print('Descriptive Statistics for Item prices by Store')\n",
    "prices_df.groupby(['store_id','item_id'])['sell_price'].agg(['min', 'max', 'mean', 'count']).head(20)\n",
    "\n",
    "# Zeros Distribution\n",
    "\n",
    "data_sample = sales_train_validation.loc[:,days].T\n",
    "dist = (data_sample==0).sum()\n",
    "hist_data = [dist]\n",
    "group_labels = ['Zero Sale distribution']\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.update_layout(autosize=False, width=800, height=500, margin=dict(l=10, r=10, b=10, t=40))\n",
    "fig.update_layout(template='plotly_dark', title_text='Zero Sale Days distribution')\n",
    "fig.show()\n",
    "\n",
    "# Ones sales distribution\n",
    "data_sample = sales_train_validation.loc[:,days].T\n",
    "dist = (data_sample==1).sum()\n",
    "hist_data = [dist]\n",
    "group_labels = ['One Sale distribution']\n",
    "fig = ff.create_distplot(hist_data, group_labels)\n",
    "fig.update_layout(autosize=False, width=800, height=500, margin=dict(l=10, r=10, b=10, t=40))\n",
    "fig.update_layout(template='plotly_dark', title_text='One Sale Days distribution')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# Interactive viz\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "\n",
    "df = pd.read_csv(INPUT_DIR_PATH + 'sales_train_evaluation.csv')\n",
    "\n",
    "sales_sum_by_store = df.groupby(['store_id']).sum().T.reset_index(drop = True)\n",
    "sales_mean_by_store = df.groupby(['store_id']).mean().T.reset_index(drop = True) \n",
    "\n",
    "print('Sales aggregated by different Stores')\n",
    "df.iplot(kind='box',  margin=(10, 10, 10, 40) ,dimensions=(900,500), title = 'Total Sales by Store ID', xTitle = 'Store ID', yTitle = 'Total Sales')\n",
    "df.iplot(kind='box',  margin=(10, 10, 10, 40) ,dimensions=(900,500), title = 'Average Sales by Store ID', xTitle = 'Store ID', yTitle = 'Average Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw\n",
    "df = df.groupby('date').sum()\n",
    "df['sales'].T.plot(figsize=(30,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adding exonerous variables:\n",
    "- Feature Engineering\n",
    "- Events and holidays\n",
    "\n",
    "Note: similar to a new feature in regular ML models\n",
    "\n",
    "Date       |  Y  |   Weekday   |   Avg_sales_7_lags   |   price_increased   |   is_game_night   |   (...)  |\n",
    "____________________________________________________________________________________________________________\n",
    "1-1-2014   |  7  |      2      |          6.2         |        'y'          |        'n'        |   (...)  |\n",
    "2-1-2014   |  6  |      3      |          6.1         |        'n'          |        'y'        |   (...)  |\n",
    "\n",
    "  (...)    |(...)|    (...)    |         (...)        |        (...)        |        (...)      |   (...)  |\n",
    "____________________________________________________________________________________________________________  \n",
    "1-5-2020.  |  ?  |      6      |          7.2         |        'n'          |        'y'        |   (...)  |\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "years = [year for year in range(2012, 2021)]\n",
    "\n",
    "us_holidays = holidays.US(state='CA', years = years) + holidays.US(state='TX', years = years) + holidays.US(state='WI', years = years)\n",
    "\n",
    "# Customized holidays:\n",
    "\n",
    "customized_holidays = {\"2020-06-29\": \"Today's day\",\n",
    "                      \"2020-06-30\": \"International day of Tomorrow\"}\n",
    "\n",
    "# us_holidays\n",
    "us_holidays.append(customized_holidays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature Engineering\n",
    "\n",
    "Dates, pricesm etc, of previous X days, same category, same state, etc..\n",
    "\"\"\"\n",
    " # rolling demand features\n",
    "    \n",
    "for val in range(1, 30):\n",
    "    df[f\"shift_t{val}\"] = df.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(val))\n",
    "for val in range(1, 30):\n",
    "    df[f\"rolling_std_t{val}\"] = df.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(val).std())\n",
    "for val in range(1, 30):\n",
    "    df[f\"rolling_mean_t{val}\"] = df.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(val).mean())\n",
    "\n",
    "df[\"rolling_skew_t30\"] = df.groupby([\"id\"])[\"demand\"].transform( lambda x: x.shift(28).rolling(30).skew())\n",
    "df[\"rolling_kurt_t30\"] = df.groupby([\"id\"])[\"demand\"].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "\n",
    "# price features\n",
    "df['lag_price_t1'] = df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "df['price_change_t1'] = (df['lag_price_t1'] - df['sell_price']) / (df['lag_price_t1'])\n",
    "df['rolling_price_max_t365'] = df.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "df['price_change_t365'] = (df['rolling_price_max_t365'] - df['sell_price']) / (df['rolling_price_max_t365'])\n",
    "df['rolling_price_std_t7'] = df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "df['rolling_price_std_t30'] = df.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "df.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n",
    "    \n",
    "# time features\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "attrs = [\"year\", \"quarter\", \"month\", \"week\", \"day\", \"dayofweek\", \"is_year_end\", \"is_year_start\", \"is_quarter_end\", \\\n",
    "    \"is_quarter_start\", \"is_month_end\",\"is_month_start\",\n",
    "]\n",
    "\n",
    "for attr in attrs:\n",
    "    dtype = np.int16 if attr == \"year\" else np.int8\n",
    "    df[attr] = getattr(df['date'].dt, attr).astype(dtype)\n",
    "df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "COLUMN_1 = [14, 30, 61, 121, 182, 365] # [14, 30, 60, 120, 180, 360] -> it is what everyobe uses, doesnt make sense...\n",
    "COLUMN_2 = [7, 14, 28, 56]\n",
    "for i in COLUMN_1:\n",
    "    df[f'rolling_mean{i}'] = df.groupby(['id'])['d_1'].transform(lambda x: x.shift(28).rolling(i).mean())    \n",
    "    df[f'rolling_std{i}'] = df.groupby(['id'])['d_1'].transform(lambda x: x.shift(28).rolling(i).std()) \n",
    "\n",
    "for i in range(1, 58):\n",
    "    df['lag_t14'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: x.shift(14))\n",
    "    df['lag_t7'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: x.shift(7))\n",
    "    df['lag_t28'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: x.shift(28))\n",
    "    df['lag_t56'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: x.shift(56))\n",
    "    df['stddev'] = df.groupby(['id'])[f'd_{i}'].transform('std')\n",
    "    df['lag_t14'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.std(x.shift(14)))\n",
    "    df['lag_t7'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.std(x.shift(7)))\n",
    "    df['lag_t28'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.std(x.shift(28)))\n",
    "    df['lag_t56'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.std(x.shift(56)))\n",
    "\n",
    "for i in range(1, 58):\n",
    "    df['fourier_lag_t14'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.fft.fft(x.shift(14)))\n",
    "    df['fourier_lag_t7'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.fft.fft(x.shift(7)))\n",
    "    df['fourier_lag_t28'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.fft.fft(x.shift(28)))\n",
    "    df['fourier_lag_t56'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.fft.fft(x.shift(56)))\n",
    "\n",
    "    df['ifourier_lag_t14'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.fft.ifft(x.shift(14)))\n",
    "    df['ifourier_lag_t7'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.fft.ifft(x.shift(7)))\n",
    "    df['ifourier_lag_t28'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.fft.ifft(x.shift(28)))\n",
    "    df['ifourier_lag_t56'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: np.fft.ifft(x.shift(56)))\n",
    "\n",
    "    df['dfourier_lag_t14'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.dct(x.shift(14), type=2, norm='ortho'))\n",
    "    df['dfourier_lag_t7'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.dct(x.shift(7), type=2, norm='ortho'))\n",
    "    df['dfourier_lag_t28'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.dct(x.shift(28), type=2, norm='ortho'))\n",
    "    df['dfourier_lag_t56'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.dct(x.shift(56), type=2, norm='ortho'))\n",
    "\n",
    "    df['idfourier_lag_t14'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.idct(x.shift(14), type=2, norm='ortho'))\n",
    "    df['idfourier_lag_t7'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.idct(x.shift(7), type=2, norm='ortho'))\n",
    "    df['idfourier_lag_t28'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.idct(x.shift(28), type=2, norm='ortho'))\n",
    "    df['idfourier_lag_t56'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.idct(x.shift(56), type=2, norm='ortho'))\n",
    "\n",
    "    df['dsfourier_lag_t14'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.dst(x.shift(14), type=2, norm='ortho'))\n",
    "    df['dsfourier_lag_t7'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.dst(x.shift(7), type=2, norm='ortho'))\n",
    "    df['dsfourier_lag_t28'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.dst(x.shift(28), type=2, norm='ortho'))\n",
    "    df['dsfourier_lag_t56'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.dst(x.shift(56), type=2, norm='ortho'))\n",
    "\n",
    "    df['idfourier_lag_t14'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.idst(x.shift(14), type=2, norm='ortho'))\n",
    "    df['idfourier_lag_t28'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.idst(x.shift(28), type=2, norm='ortho'))\n",
    "    df['idfourier_lag_t56'] = df.groupby(['id'])[f'd_{i}'].transform(lambda x: scipy.fft.idst(x.shift(56), type=2, norm='ortho'))\n",
    "    \n",
    "df['price_max'] = df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "df['price_min'] = df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "df['price_std'] = df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "df['price_mean'] = df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "\n",
    "# percentage change between the current and a prior element\n",
    "df[\"sell_price_rel_diff\"] = df.groupby([\"item_id\"])[\"sell_price\"].pct_change()\n",
    "\n",
    "# rolling std of prices\n",
    "df[\"sell_price_roll_sd7\"] = df.groupby([\"item_id\"])[\"sell_price\"].transform(lambda x: x.rolling(7).std())\n",
    "\n",
    "# relative cumulative price \n",
    "grouped = df.groupby([\"item_id\"])[\"sell_price\"]\n",
    "df[\"sell_price_cumrel\"] = (grouped.shift(0) - grouped.cummin()) / (1 + grouped.cummax() - grouped.cummin())\n",
    "\n",
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "\n",
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week \n",
    "# by month mean\n",
    "# by year mean\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encodings\n",
    "\n",
    "Add:\n",
    "Circle Encoding\n",
    "Target Encoding\n",
    "Etc..\n",
    "\n",
    "\"\"\"\n",
    "def circle_encode(df, col):\n",
    "\n",
    "    maxval = float(df[col].max())\n",
    "    \n",
    "    cosval = np.cos(2 * np.pi * df[col]/maxval)\n",
    "    sinval = np.sin(2 * np.pi * df[col]/maxval)\n",
    "\n",
    "    return cosval, sinval\n",
    "\n",
    "## Adding the embedded vectors \n",
    "df['wd1'] =0\n",
    "df['wd2'] =0\n",
    "df['wd3'] =0\n",
    "df['wd4'] =0\n",
    "\n",
    "df.loc[:,'wd1'][df['weekday'] =='Sunday'] , df.loc[:,'wd2'][df['weekday'] =='Sunday'],\\\n",
    "df.loc[:,'wd3'][df['weekday'] =='Sunday'] , df.loc[:,'wd4'][df['weekday'] =='Sunday']= 0.4 ,-0.3 ,0.6,0.1\n",
    "\n",
    "df.loc[:,'wd1'][df['weekday'] =='Monday'] , df.loc[:,'wd2'][df['weekday'] =='Monday'],\\\n",
    "df.loc[:,'wd3'][df['weekday'] =='Monday'] , df.loc[:,'wd4'][df['weekday'] =='Monday']= 0.2 ,0.2 ,0.5,-0.3\n",
    "\n",
    "df.loc[:,'wd1'][df['weekday'] =='Tuesday'] ,df.loc[:,'wd2'][df['weekday'] =='Tuesday'],\\\n",
    "df.loc[:,'wd3'][df['weekday'] =='Tuesday'] , df.loc[:,'wd4'][df['weekday'] =='Tuesday']= 0.1,-1.0,1.3,0.9\n",
    "\n",
    "df.loc[:,'wd1'][df['weekday'] =='Wednesday'] , df.loc[:,'wd2'][df['weekday'] =='Wednesday'],\\\n",
    "df.loc[:,'wd3'][df['weekday'] =='Wednesday'] , df.loc[:,'wd4'][df['weekday'] =='Wednesday']= -0.6,0.5,1.2,0.7\n",
    "\n",
    "df.loc[:,'wd1'][df['weekday'] =='Thursday'] , df.loc[:,'wd2'][df['weekday'] =='Thursday'],\\\n",
    "df.loc[:,'wd3'][df['weekday'] =='Thursday'] , df.loc[:,'wd4'][df['weekday'] =='Thursday']= 0.9,0.2,-0.1,0.6\n",
    "\n",
    "df.loc[:,'wd1'][df['weekday'] =='Friday'] , df.loc[:,'wd2'][df['weekday'] =='Friday'],\\\n",
    "df.loc[:,'wd3'][df['weekday'] =='Friday'] , df.loc[:,'wd4'][df['weekday'] =='Friday']= 0.4,1.1,0.3,-1.5\n",
    "\n",
    "\n",
    "df.loc[:,'wd1'][df['weekday'] =='Saturday'] , df.loc[:,'wd2'][df['weekday'] =='Saturday'],\\\n",
    "df.loc[:,'wd3'][df['weekday'] =='Saturday'] , df.loc[:,'wd4'][df['weekday'] =='Saturday']= 0.3,-0.2,0.6,0.0\n",
    "\n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "calendar_df = encode_categorical(calendar_df, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]).pipe(reduce_mem_usage)\n",
    "sales_train_validation_df = encode_categorical(sales_train_validation_df, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]).pipe(reduce_mem_usage)\n",
    "sell_prices_df = encode_categorical(sell_prices_df, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preparation\n",
    "\n",
    "1. Null  -> Do nothing\n",
    "2. Zeros and negatives-> Convert to NA\n",
    "3. Outliers - Explain or convert to NA\n",
    "4. Transformations\n",
    "\n",
    "\"\"\"\n",
    "df['sales'] = df['sales'].replace(0, np.nan)  #not working........\n",
    "# or\n",
    "# df['y'] = np.where(df['y'] <=0, np.nan)    #not working........\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "df = df[df['sales'].between(df['sales'].quantile(.05), df['sales'].quantile(.95))]\n",
    "df['sales'].T.plot(figsize=(30,5))\n",
    "\n",
    "\n",
    "nan_features_cat = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "for feature in nan_features_cat:\n",
    "    df[feature].fillna('unknown', inplace = True)\n",
    "\n",
    "data['sell_price'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check if the wave is stationary\n",
    "\n",
    "Needs to have constant mean (dont have any trend related to time)\n",
    "Constant variance (homoscedascity, amplitude doesn't change with time)\n",
    "Constant autocorrelation (frequency of the wave should be constant)\n",
    "\n",
    "Dickey-Fuller test says that if 'Test Statistic' is greater than the 'Critical Value', it means the time series is stationary.\n",
    "\n",
    "\n",
    "The smaller the p-value, the more likely to be stationary.\n",
    "\n",
    "To get stationary data, we can apply transformations like log, differencing, etc..\n",
    "\"\"\"\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "#1\n",
    "adf, pvalue, usedlag_, nobs_, critical_values_, icbest_ = adfuller(df['sales'])\n",
    "\n",
    "print(\"\")\n",
    "print(f'adf (Test Statistic) is: {adf}')\n",
    "print(f'p-value is: {pvalue}')\n",
    "print(f'usedlag_ is: {usedlag_}')\n",
    "print(f'nobs_ is: {nobs_}')\n",
    "print(f'critical_values_ is: {critical_values_}')\n",
    "print(f'icbest_ is: {icbest_}')\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#2\n",
    "res = sm.tsa.adfuller(df['sales'].dropna(),regression='ct')\n",
    "print('p-value:{}'.format(res[1]))\n",
    "\n",
    "res = sm.tsa.adfuller(df['sales'].diff().dropna(),regression='c')\n",
    "print('p-value:{}'.format(res[1]))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "#3\n",
    "def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n",
    "    \"\"\"\n",
    "        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n",
    "        \n",
    "        y - timeseries\n",
    "        lags - how many lags to include in ACF, PACF calculation\n",
    "    \"\"\"\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style):    \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "tsplot(df['sales'], lags=60)\n",
    "\n",
    "# Augmented Dickey-Fuller test #4\n",
    "adf1 = adfuller(df, autolag='AIC')\n",
    "print(\"p-value of Foods serie is: {}\".format(float(adf1[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sm.tsa.seasonal_decompose(df['sales'],period=1) # , model='multiplicative'\n",
    "fig = res.plot()\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Denoising the signal using Wavelet\n",
    "\"\"\"\n",
    "\n",
    "def maddest(d, axis=None):\n",
    "    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n",
    "\n",
    "def denoise_signal(x, wavelet='db4', level=1):\n",
    "    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n",
    "    sigma = (1/0.6745) * maddest(coeff[-level])\n",
    "\n",
    "    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n",
    "    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n",
    "\n",
    "    return pywt.waverec(coeff, wavelet, mode='per')\n",
    "\n",
    "wavelet_y = denoise_signal(df['sales'])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(30, 20))\n",
    "\n",
    "ax[0, 0].plot(df['sales'], color='seagreen', marker='o') \n",
    "ax[0, 0].set_title('Original Sales', fontsize=24)\n",
    "ax[0, 1].plot(wavelet_y, color='red', marker='.') \n",
    "ax[0, 1].set_title('After Wavelet Denoising', fontsize=24)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "df['sales'] = wavelet_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelet_y.shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Average Smoothing to reduce more noise\n",
    "\"\"\"\n",
    "\n",
    "def average_smoothing(signal, kernel_size=3, stride=1):\n",
    "    sample = []\n",
    "    start = 0\n",
    "    end = kernel_size\n",
    "    while end <= len(signal):\n",
    "        start = start + stride\n",
    "        end = end + stride\n",
    "        sample.extend(np.ones(end - start)*np.mean(signal[start:end]))\n",
    "    return np.array(sample)\n",
    "\n",
    "\n",
    "smoothing_y = average_smoothing(df['sales'])\n",
    "\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(df['sales'])), y=df['sales'], showlegend=False,\n",
    "                    mode='lines+markers', name=\"First sample\",\n",
    "                         marker=dict(color=\"mediumseagreen\")),\n",
    "             row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(df['sales'])), y=smoothing_y, showlegend=False,\n",
    "                    mode='lines+markers', name=\"First sample\",\n",
    "                         marker=dict(color=\"black\")),\n",
    "             row=1, col=1)\n",
    "\n",
    "# df['sales'] = smoothing_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To create models and optimize the SARIMAX parameters\n",
    "Split in Train/Validation\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "train = df['y']\n",
    "\n",
    "fig, ax = plt.subplots(2,1)\n",
    "fig = sm.graphics.tsa.plot_acf(train, lags=50, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(train, lags=50, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Auto tune parameters\n",
    "\n",
    "How to evaluate the best.. MSE or AIC?\n",
    "\n",
    "\n",
    "p is the first most significant negative value in PACF\n",
    "d\n",
    "q is the first most significant negative value in ACF\n",
    "\"\"\"\n",
    "import itertools\n",
    "#set parameter range\n",
    "p = range(0,3)\n",
    "q = range(1,3)\n",
    "d = range(1,2)\n",
    "s = range(7,8)\n",
    "\n",
    "# list of all parameter combos\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = list(itertools.product(p, d, q, s))\n",
    "\n",
    "results = []\n",
    "best_aic = float(\"inf\")\n",
    "\n",
    "print(pdq)\n",
    "print(seasonal_pdq)\n",
    "# SARIMA model pipeline\n",
    "for param in tqdm_notebook(pdq):\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(endog=df['sales'],\n",
    "                                    order=param,\n",
    "                                    seasonal_order=param_seasonal,\n",
    "                                    enforce_stationary=False,\n",
    "                                    enforce_invertibility=False,\n",
    "                                    time_varying_regression = False,\n",
    "                                    mle_regression = True)\n",
    "            result = mod.fit(max_iter = 50, method = 'powell')\n",
    "            results.append(result)\n",
    "            print('SARIMA{},{} - AIC:{}'.format(param, param_seasonal, result.aic))\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        aic = model.aic\n",
    "        # saving best model, AIC and parameters\n",
    "        if aic < best_aic:\n",
    "            best_model = model\n",
    "            best_aic = aic\n",
    "            best_param = param\n",
    "        results.append([param, model.aic])\n",
    "\n",
    "result_table = pd.DataFrame(results)\n",
    "result_table.columns = ['parameters', 'aic']\n",
    "# sorting in ascending order, the lower AIC is - the better\n",
    "result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n",
    "result_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According Peterson, T. (2014) the AIC (Akaike information criterion) is an estimator of the relative quality of statistical models for a given set of data. \n",
    "# Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. The low AIC value the better.\n",
    "\n",
    "print(result_table.parameters[0])\n",
    "\n",
    "best_model=sm.tsa.statespace.SARIMAX(df['demand'], \n",
    "                                     order=(p, d, q),\n",
    "                                     seasonal_order=(P, D, Q, s)).fit(disp=-1)\n",
    "print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having a low autocorrelation in resid means that ou model is good\n",
    "\n",
    "\n",
    "res = best_model.resid\n",
    "fig,ax = plt.subplots(2,1,figsize=(15,8))\n",
    "fig = sm.graphics.tsa.plot_acf(res, lags=50, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(res, lags=50, ax=ax[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_sarima = np.mean(np.abs(res))\n",
    "print('Mean absolute error: ', mae_sarima)\n",
    "result.summary()\n",
    "result.plot_diagnostics(figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this model's summary, Jarque-Bera test's Prob is under 0.05.\n",
    "# It means that this model's resid is not following a normal distribution.\n",
    "# In other words, some infomations still remain in this model's resid.\n",
    "\n",
    "# Also, look at skewness in the histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- In this model's summary, Jarque-Bera test's Prob is under 0.05.\n",
    "It means that this model's resid is not following a normal distribution.\n",
    "In other words, some infomations still remain in this model's resid. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "pred = result.predict()[1:]\n",
    "print('ARIMA model MSE:{}'.format(mean_squared_error(tes,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training model\n",
    "\"\"\"\n",
    "\n",
    "p =\n",
    "d =\n",
    "q =\n",
    "S =\n",
    "\n",
    "model = sm.tsa.statespace.SARIMAX(train, order=(p,d,q), \n",
    "                                  seasonal_order(p,d,q,S),\n",
    "                                 holidays)\n",
    "                                 .fit(max_iter=50, method='powell')\n",
    "res = model.resid\n",
    "fig, ax = plt.subplots(2,1)\n",
    "fig = sm.graphics.tsa.plot_acf(res, lags=50, ax=ax[0])\n",
    "fig = sm.graphics.tsa.plot_pacf(res, lags=50, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use Walk Forward Validation to validate the model\n",
    "\"\"\"\n",
    "\n",
    "# data = entire dataset\n",
    "# n_test = point where data is split into training and test sets\n",
    "def walk_forward_validation(data, n_test):\n",
    "    predictions = np.array([])\n",
    "    mape_list = []\n",
    "    train, test = data[:n_test], data[n_test:]\n",
    "    day_list = [7,14,21,28] # weeks 1,2,3,4\n",
    "    for i in day_list:\n",
    "        # Fit model to training data\n",
    "        model = sm.tsa.statespace.SARIMAX(train, order=(1,1,2), seasonal_order=(1,1,2,7)).fit(max_iter = 50,\n",
    "                                          method = 'powell')\n",
    "        \n",
    "        # Forecast daily loads for week i\n",
    "        forecast = model.get_forecast(steps = 7)\n",
    "        predictions = np.concatenate(predictions, forecast, \n",
    "                                     axis=None)\n",
    "        # Calculate MAPE and add to mape_list\n",
    "        j = i-7\n",
    "        mape_score = (abs(test[j:i]-predictions[j:i])/test[j:i])*100\n",
    "        mape_mean = mape_score.mean()\n",
    "        mape_list.append(mape_mean)\n",
    "        # Add week i to training data for next loop\n",
    "        train = np.concatenate((train, test[j:i]), axis=None)\n",
    "    return predictions, mape_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, mape_list = walk_forward_validation(df['y'], 1913)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions example\n",
    "\n",
    "train_dataset = df[:-56]\n",
    "val_dataset = df[-56:]\n",
    "\n",
    "predictions = []\n",
    "for row in tqdm(train_dataset[train_dataset.columns[-30:]].values):\n",
    "    fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0, 1, 1, 7), initialization='approximate_diffuse').fit()\n",
    "    predictions.append(fit.forecast(3))\n",
    "predictions = np.array(predictions)\n",
    "error_arima = np.linalg.norm(predictions - val_dataset.values)/len(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool Details\n",
    "\n",
    "Analysing stockouts\n",
    "https://www.kaggle.com/jpmiller/grouping-items-by-stockout-pattern\n",
    "\n",
    "Wavelet Denoising\n",
    "https://www.kaggle.com/eswarchandt/timeseries-forecasting-accuracy\n",
    "\n",
    "Advanced Transformations\n",
    "https://www.kaggle.com/rdizzl3/time-series-transformations\n",
    "\n",
    "Croston Model\n",
    "https://www.kaggle.com/siavrez/simple-eda-with-croston-method\n",
    "\n",
    "WRMSSE Evaluation\n",
    "https://www.kaggle.com/sibmike/fast-clear-wrmsse-18ms\n",
    "\n",
    "LGBM GroupKFold CV\n",
    "https://www.kaggle.com/ragnar123/simple-lgbm-groupkfold-cv\n",
    "\n",
    "Dates Feataure Engineering\n",
    "https://www.kaggle.com/raenish/cheatsheet-date-helpers\n",
    "\n",
    "All ML models\n",
    "https://www.kaggle.com/rodrigolima82/time-series-analysis-es-sarimax-xgb-lgbm#Kaggle-Competition:-M5-Forecasting\n",
    "\n",
    "MAPA methodology\n",
    "https://www.kaggle.com/brunoborges95/m5-time-series-forecasting-using-mapa-sarimax\n",
    "\n",
    "Deep Neural Nets approach\n",
    "https://www.kaggle.com/timetraveller98/m5-accuracy-pytorch-deep-neural-net-dnn\n",
    "\n",
    "LSTM Simple approach\n",
    "https://www.kaggle.com/eakdag/lstm-with-keras-0-7\n",
    "\n",
    "Prophet approach\n",
    "https://www.kaggle.com/ritvik29/fb-prophet-compile-codes-june16\n",
    "\n",
    "Clustering Items\n",
    "https://www.kaggle.com/ry2m2t/m5-clustering-items-dft-pca-t-sne-dbscan-en-jp\n",
    "\n",
    "TBATS to use multiple seasonalities in SARIMAX\n",
    "https://medium.com/intive-developers/forecasting-time-series-with-multiple-seasonalities-using-tbats-in-python-398a00ac0e8a\n",
    "\n",
    "\n",
    "Parallelize training\n",
    "\n",
    "Interactive plots\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Why ML is not so well suited as arima? eg Gauss Markov Theorem\n",
    "\n",
    "In LGBM convert df to matrix before training \n",
    "from scipy import sparse \n",
    "temp_matrix = sparse.csr_matrix(temp_df)\n",
    "\n",
    "Pipeline\n",
    "\n",
    "Import datasets, reduce mem, merge them\n",
    "Data cleaning\n",
    "Making the wave stationary ('sales'), using\n",
    "Feature engineering\n",
    "Encoding\n",
    "Feature transformations, normalizations\n",
    "Training\n",
    "Validating\n",
    "HP optimization\n",
    "Predicting\n",
    "\n",
    "\n",
    "Features\n",
    "is_vespera_feriado\n",
    "consecutive_zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1941)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "train = pd.read_csv('sales_train_evaluation.csv')\n",
    "train['date'] = pd.to_datetime(train['date'], format=\"%Y-%m-%d\")\n",
    "\n",
    "train_df = train[train['store']==1]\n",
    "train_df = train_df[train['item']==1]\n",
    "# train_df = train_df.set_index('date')\n",
    "train_df['year'] = train['date'].dt.year\n",
    "train_df['month'] = train['date'].dt.month\n",
    "train_df['day'] = train['date'].dt.dayofyear\n",
    "train_df['weekday'] = train['date'].dt.weekday\n",
    "\n",
    "train_df = train_df.set_index('date')\n",
    "train_df['sales'] = train_df['sales'].astype(float)\n",
    "df = df.groupby(['date', 'id']).sum().reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>store_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date store_id        item_id  sales\n",
       "0 2011-01-29     CA_1  HOBBIES_1_008     12\n",
       "1 2011-01-30     CA_1  HOBBIES_1_008     15\n",
       "2 2011-01-31     CA_1  HOBBIES_1_008      0\n",
       "3 2011-02-01     CA_1  HOBBIES_1_008      0\n",
       "4 2011-02-02     CA_1  HOBBIES_1_008      0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw\n",
    "df['date'] = pd.to_datetime(df['date'], format=\"%Y-%m-%d\")\n",
    "df = df[df['store_id']=='CA_1']\n",
    "df = df[df['item_id']=='HOBBIES_1_008']\n",
    "df = df[['date', 'store_id', 'item_id', 'sales']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>store_id_CA_1</th>\n",
       "      <th>item_id_HOBBIES_1_008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  sales  store_id_CA_1  item_id_HOBBIES_1_008\n",
       "0 2011-01-29     12              1                      1\n",
       "1 2011-01-30     15              1                      1\n",
       "2 2011-01-31      0              1                      1\n",
       "3 2011-02-01      0              1                      1\n",
       "4 2011-02-02      0              1                      1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>sales</th>\n",
       "      <th>store_id_CA_1</th>\n",
       "      <th>item_id_HOBBIES_1_008</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-02-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  sales  store_id_CA_1  item_id_HOBBIES_1_008  year  month  day  \\\n",
       "0 2011-01-29     12              1                      1  2011      1   29   \n",
       "1 2011-01-30     15              1                      1  2011      1   30   \n",
       "2 2011-01-31      0              1                      1  2011      1   31   \n",
       "3 2011-02-01      0              1                      1  2011      2   32   \n",
       "4 2011-02-02      0              1                      1  2011      2   33   \n",
       "\n",
       "   weekday  \n",
       "0        5  \n",
       "1        6  \n",
       "2        0  \n",
       "3        1  \n",
       "4        2  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.dayofyear\n",
    "df['weekday'] = df['date'].dt.weekday\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>store_id_CA_1</th>\n",
       "      <th>item_id_HOBBIES_1_008</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-30</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-02-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sales  store_id_CA_1  item_id_HOBBIES_1_008  year  month  day  \\\n",
       "date                                                                        \n",
       "2011-01-29   12.0              1                      1  2011      1   29   \n",
       "2011-01-30   15.0              1                      1  2011      1   30   \n",
       "2011-01-31    0.0              1                      1  2011      1   31   \n",
       "2011-02-01    0.0              1                      1  2011      2   32   \n",
       "2011-02-02    0.0              1                      1  2011      2   33   \n",
       "\n",
       "            weekday  \n",
       "date                 \n",
       "2011-01-29        5  \n",
       "2011-01-30        6  \n",
       "2011-01-31        0  \n",
       "2011-02-01        1  \n",
       "2011-02-02        2  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.set_index('date')\n",
    "df['sales'] = df['sales'].astype(float)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1941 entries, 2011-01-29 to 2016-05-22\n",
      "Columns: 508 entries, store_id_CA_1 to forecast_unknown\n",
      "dtypes: int64(4), uint8(504)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "exog_data = df.drop(columns=['sales']).fillna('unknown')\n",
    "exog_data = pd.get_dummies(exog_data)\n",
    "exog_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  warnings.warn('No frequency information was'\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:159: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  warnings.warn('No frequency information was'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      SARIMAX Results                                      \n",
      "===========================================================================================\n",
      "Dep. Variable:                               sales   No. Observations:                 1941\n",
      "Model:             SARIMAX(6, 1, 0)x(0, 1, [1], 7)   Log Likelihood               -8092.663\n",
      "Date:                             Thu, 02 Jul 2020   AIC                          16213.326\n",
      "Time:                                     09:12:37   BIC                          16291.262\n",
      "Sample:                                 01-29-2011   HQIC                         16241.992\n",
      "                                      - 05-22-2016                                         \n",
      "Covariance Type:                               opg                                         \n",
      "=========================================================================================\n",
      "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------\n",
      "store_id_CA_1         -1.084e-05      0.173  -6.28e-05      1.000      -0.339       0.339\n",
      "item_id_HOBBIES_1_008 -1.828e-05      0.174     -0.000      1.000      -0.341       0.341\n",
      "year                   3497.8642    360.223      9.710      0.000    2791.840    4203.888\n",
      "month                     1.8952      1.214      1.561      0.119      -0.485       4.275\n",
      "day                       9.5027      0.991      9.593      0.000       7.561      11.444\n",
      "weekday                1.416e-05      0.238   5.95e-05      1.000      -0.467       0.467\n",
      "ar.L1                    -0.0769      0.008     -9.186      0.000      -0.093      -0.060\n",
      "ar.L2                    -0.0162      0.005     -3.601      0.000      -0.025      -0.007\n",
      "ar.L3                     0.0040      0.014      0.286      0.775      -0.023       0.031\n",
      "ar.L4                    -0.0057      0.020     -0.287      0.774      -0.044       0.033\n",
      "ar.L5                     0.0136      0.007      2.069      0.039       0.001       0.026\n",
      "ar.L6                     0.0408      0.004      9.176      0.000       0.032       0.050\n",
      "ma.S.L7                  -0.0889      0.010     -8.814      0.000      -0.109      -0.069\n",
      "sigma2                  212.7910      5.617     37.883      0.000     201.782     223.800\n",
      "===================================================================================\n",
      "Ljung-Box (Q):                      792.04   Jarque-Bera (JB):              2453.65\n",
      "Prob(Q):                              0.00   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.36   Skew:                             0.43\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                         8.45\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "[2] Covariance matrix is singular or near-singular, with condition number 1.71e+22. Standard errors may be unstable.\n"
     ]
    }
   ],
   "source": [
    "start_index = '2017-10-01'\n",
    "end_index = '2017-12-31'\n",
    "\n",
    "sarimax_mod6 = sm.tsa.statespace.SARIMAX(endog = df['sales'][:start_index],\n",
    "                                        exog = exog_data[:start_index],  \n",
    "                                        trend='n', order=(6,1,0), seasonal_order=(0,1,1,7)).fit()\n",
    "print(sarimax_mod6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NormaltestResult(statistic=4980.0386128802265, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import normaltest\n",
    "resid = sarimax_mod6.resid\n",
    "print(normaltest(resid))\n",
    "\n",
    "\n",
    "# If pvalue of reuslt is very small it means that the residual is not a normal distribution\n",
    "# If we observe a correlation between ACF and PACF it means we have seaasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/statsmodels/tsa/statespace/kalman_filter.py:2012: ValueWarning: Dynamic prediction specified to begin during out-of-sample forecasting period, and so has no effect.\n",
      "  warn('Dynamic prediction specified to begin during'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ffda8ae34c0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_index = '2017-10-01'\n",
    "end_index = '2017-12-31'\n",
    "# sarimax_mod6.forecast(steps = 121,exog = exog_data[start_index:end_index])\n",
    "\n",
    "df['forecast'] = sarimax_mod6.predict(start = start_index, \n",
    "                                      end= end_index, \n",
    "#                                       exog = exog_data[start_index:end_index], \n",
    "                                      dynamic= True)  \n",
    "\n",
    "df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE: nan % \n",
      "SMAPE: 0.00 %\n"
     ]
    }
   ],
   "source": [
    "mape = np.mean(abs((df['sales']-df['forecast'])/df['sales']))*100\n",
    "smape = np.mean((np.abs(df['forecast'] - df['sales']) * 200/ (np.abs(df['forecast']) + np.abs(df['forecast']))).fillna(0))\n",
    "print('MAPE: %.2f %% \\nSMAPE: %.2f'% (mape,smape), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e852fae093df41aea979ee398ac2462a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1911.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for ARMA and trend. All parameters except for variances will be set to zeros.\n",
      "  warn('Too few observations to estimate starting parameters%s.'\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/statsmodels/tsa/statespace/sarimax.py:866: UserWarning: Too few observations to estimate starting parameters for seasonal ARMA. All parameters except for variances will be set to zeros.\n",
      "  warn('Too few observations to estimate starting parameters%s.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1911,3) (28,8) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-f5506648a5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0merror_arima\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1911,3) (28,8) "
     ]
    }
   ],
   "source": [
    "# Predictions example\n",
    "\n",
    "train_dataset = df[:-28]\n",
    "val_dataset = df[-28:]\n",
    "\n",
    "predictions = []\n",
    "for row in tqdm(train_dataset[2:][train_dataset.columns[-30:]].values):\n",
    "    fit = sm.tsa.statespace.SARIMAX(row, seasonal_order=(0, 1, 1, 7), initialization='approximate_diffuse').fit()\n",
    "    predictions.append(fit.forecast(3))\n",
    "predictions = np.array(predictions)\n",
    "error_arima = np.linalg.norm(predictions - val_dataset.values)/len(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
