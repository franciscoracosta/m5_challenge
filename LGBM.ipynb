{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LGB approach:\n",
    "\n",
    "Scaling: Not neeeded since tree based models split each feature separately (splitting from 1-1000 is equal to spliting 1-10)\n",
    "\n",
    "Outliers: (in target variable) Being a boosted model, it is sensible to outliers. Boosting focus on previous trees errors, since outliers generate a big error the model will focus a lot on that. -> Log the target variable and in the input variables set for example the average of same day past years or past week or something..\n",
    "\n",
    "Null values -> Manually infer something is the best approach (comvert to unknown to be identified as a category)\n",
    "\n",
    "Zeros-> Ok\n",
    "\n",
    "Categorical features-> Tree based models can use its own auto encoder (label or one hot). Best is to do it manually, decision should be made on cardinality of categories (>15). Try target encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try target encoder\n",
    "Try removing irrelevant features\n",
    "Add TQDM to see progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 16:03:50,187  - INFO - Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "# register_matplotlib_converters()\n",
    "# sns.set()\n",
    "\n",
    "import logging\n",
    "formatter = '%(asctime)s  - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(format=formatter)\n",
    "logger = logging.getLogger('logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.info(f'Setup complete')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    int_columns = df.select_dtypes(include=[\"int\"]).columns\n",
    "    float_columns = df.select_dtypes(include=[\"float\"]).columns\n",
    "\n",
    "    for col in int_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "\n",
    "    for col in float_columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    if verbose:\n",
    "        logger.info(\n",
    "            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n",
    "                end_mem, 100 * (start_mem - end_mem) / start_mem\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    INPUT_DIR = f\"src/data/\"\n",
    "\n",
    "    logger.info(\"Reading files...\")\n",
    "\n",
    "    calendar = pd.read_csv(f\"{INPUT_DIR}calendar.csv\").pipe(reduce_mem_usage)\n",
    "    prices = pd.read_csv(f\"{INPUT_DIR}sell_prices.csv\").pipe(reduce_mem_usage)\n",
    "    sales = pd.read_csv(f\"{INPUT_DIR}sales_train_evaluation.csv\",).pipe(reduce_mem_usage)\n",
    "    submission = pd.read_csv(f\"{INPUT_DIR}sample_submission.csv\").pipe(reduce_mem_usage)\n",
    "\n",
    "    logger.info(f\"sales shape:{sales.shape}\")\n",
    "    logger.info(f\"prices shape: {prices.shape}\")\n",
    "    logger.info(f\"calendar shape: {calendar.shape}\")\n",
    "    logger.info(f\"submission shape: {submission.shape}\")\n",
    "\n",
    "    return sales, prices, calendar, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 16:08:20,469  - INFO - Reading files...\n",
      "2020-09-16 16:08:20,489  - INFO - Mem. usage decreased to  0.12 Mb (41.9% reduction)\n",
      "2020-09-16 16:08:22,419  - INFO - Mem. usage decreased to 143.53 Mb (31.2% reduction)\n",
      "2020-09-16 16:10:20,648  - INFO - Mem. usage decreased to 95.61 Mb (78.9% reduction)\n",
      "2020-09-16 16:10:20,810  - INFO - Mem. usage decreased to  2.09 Mb (84.5% reduction)\n",
      "2020-09-16 16:10:20,810  - INFO - sales shape:(30490, 1947)\n",
      "2020-09-16 16:10:20,811  - INFO - prices shape: (6841121, 4)\n",
      "2020-09-16 16:10:20,811  - INFO - calendar shape: (1969, 14)\n",
      "2020-09-16 16:10:20,812  - INFO - submission shape: (60980, 29)\n",
      "2020-09-16 16:10:20,813  - INFO - There are 30490 time series in the dataset.\n",
      "2020-09-16 16:10:20,814  - INFO - We want to predict the next 28 days.\n",
      "2020-09-16 16:10:20,814  - INFO - Complete\n"
     ]
    }
   ],
   "source": [
    "sales, prices, calendar, submission = read_data()\n",
    "\n",
    "NUM_ITEMS = sales.shape[0]  # 30490\n",
    "DAYS_PRED = submission.shape[1] - 1  # 28\n",
    "\n",
    "logger.info(f'There are {NUM_ITEMS} time series in the dataset.')\n",
    "logger.info(f'We want to predict the next {DAYS_PRED} days.')\n",
    "logger.info(f'Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 16:10:20,823  - INFO - Features encoded succecefully...\n"
     ]
    }
   ],
   "source": [
    "# sendo variaveis categoricas de id usamos um label encoder\n",
    "def encode_categorical(df, cols):\n",
    "    for col in cols:\n",
    "        # Leave NaN as it is.\n",
    "        le = LabelEncoder()\n",
    "        not_null = df[col][df[col].notnull()]\n",
    "        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "for feature in nan_features:\n",
    "    calendar[feature].fillna('unknown', inplace = True)\n",
    "\n",
    "calendar = encode_categorical(\n",
    "    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "sales = encode_categorical(\n",
    "    sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "prices = encode_categorical(\n",
    "    prices, [\"item_id\", \"store_id\"]\n",
    ").pipe(reduce_mem_usage)\n",
    "\n",
    "logger.info(\"Features encoded succecefully...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_sales(sales, submission, start_day=0, verbose=True):\n",
    "\n",
    "    # segmentar as colunas de id e featires separadamente\n",
    "    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    evals_columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n",
    "    \n",
    "    # nesta segunda faze do desafio so queremos prever os evaluation\n",
    "    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n",
    "\n",
    "    \n",
    "    # criar uma tabela auxiliar product para fazer o melt (passar de wide a long, contrario do pivot()) e colocar a info no formato certo\n",
    "    product = sales[id_columns]\n",
    "    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\").pipe(reduce_mem_usage)\n",
    "    \n",
    "#     criar o df para fazer os predicts\n",
    "    evals.columns = evals_columns\n",
    "    evals = evals.merge(product, how=\"left\", on=\"id\")\n",
    "    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n",
    "\n",
    "#     segregar os dados para treino e os dados para predict\n",
    "    sales[\"part\"] = \"train\"\n",
    "    evals[\"part\"] = \"evaluation\"\n",
    "\n",
    "#     juntar ambos os datasets de treino e de evals para fazer todas as transformacoes, f eng, etc..\n",
    "    data = pd.concat([sales, evals], axis=0)\n",
    "\n",
    "    logger.info('Dataframe created')\n",
    "    del sales, evals\n",
    "\n",
    "#     tornar a coluna de id do dia em int para separar os datasets\n",
    "    data[\"d\"] = data[\"d\"].str.slice(2,).astype(np.int16)\n",
    "    data = data[data[\"d\"] >= start_day]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def merge_calendar(data, calendar):\n",
    "    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n",
    "    return data.merge(calendar, how=\"left\", on=\"d\")\n",
    "\n",
    "def merge_prices(data, prices):\n",
    "    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 16:10:31,493  - INFO - Mem. usage decreased to 3273.49 Mb (0.0% reduction)\n",
      "2020-09-16 16:10:49,697  - INFO - Dataframe created\n",
      "2020-09-16 16:11:34,390  - INFO - Calendar merged.\n",
      "2020-09-16 16:11:43,610  - INFO - Prices merged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-16 16:11:47,698  - INFO - Mem. usage decreased to 2578.77 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "#consideramos apenas os ultimos 2 anos, se considerarmos todo os dados hist fica muito grande...\n",
    "data = reshape_sales(sales, submission, start_day =1941 - (2*52*7 + 1)) \n",
    "del sales\n",
    "\n",
    "calendar[\"d\"] = calendar[\"d\"].str.slice(2,).astype(np.int16)\n",
    "data = merge_calendar(data, calendar)\n",
    "logger.info(f'Calendar merged.')\n",
    "del calendar\n",
    "\n",
    "data = merge_prices(data, prices)\n",
    "logger.info(f'Prices merged.')\n",
    "del prices\n",
    "gc.collect()\n",
    "\n",
    "data = reduce_mem_usage(data)\n",
    "# data.head()\n",
    "# data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('M5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering \n",
    "\n",
    "def add_demand_features(df):\n",
    "\n",
    "#     get the demand of same weekday in past weeks\n",
    "    for shift in [7, 14, 28, 56, 364]:\n",
    "        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(shift)\n",
    "        )\n",
    "    logger.info(\"lag features done\")\n",
    "    gc.collect()\n",
    "\n",
    "#     get some basic statistic values for 'weekly', 'biweekly' and 'mpnthly'in previous month\n",
    "    diff = 28\n",
    "    for window in [7, 14, 28]:\n",
    "        df[f\"shift_t{diff}_rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).std()\n",
    "        )\n",
    "        df[f\"shift_t{diff}_rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).mean()\n",
    "        )\n",
    "        df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).min()\n",
    "        )\n",
    "        df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).max()\n",
    "        )\n",
    "        df[f\"rolling_sum_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).sum()\n",
    "        )\n",
    "        df[f\"rolling_skew_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).skew()\n",
    "        )\n",
    "        df[f\"rolling_kurt_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n",
    "            lambda x: x.shift(diff).rolling(window).kurt()\n",
    "        )\n",
    "            \n",
    "    logger.info(\"rolling windows features done\")\n",
    "    \n",
    "    logger.info(\"expanding windows features done\")\n",
    "           \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_price_features(df):\n",
    "            \n",
    "    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1)\n",
    "    )\n",
    "    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) / (\n",
    "        df[\"shift_price_t1\"]\n",
    "    )\n",
    "    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.shift(1).rolling(365).max()\n",
    "    )\n",
    "    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) / (\n",
    "        df[\"rolling_price_max_t365\"]\n",
    "    )\n",
    "\n",
    "    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(7).std()\n",
    "    )\n",
    "    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n",
    "        lambda x: x.rolling(30).std()\n",
    "    )\n",
    "    \n",
    "    df['price_max'] = df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "    df['price_min'] = df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "    df['price_std'] = df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "    df['price_mean'] = df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "    \n",
    "    # percentage change between the current and a prior element\n",
    "    df[\"sell_price_rel_diff\"] = df.groupby([\"item_id\"])[\"sell_price\"].pct_change()\n",
    "\n",
    "    # rolling std of prices\n",
    "    df[\"sell_price_roll_sd7\"] = df.groupby([\"item_id\"])[\"sell_price\"].transform(lambda x: x.rolling(7).std())\n",
    "\n",
    "    # relative cumulative price \n",
    "    grouped = df.groupby([\"item_id\"])[\"sell_price\"]\n",
    "    df[\"sell_price_cumrel\"] = (grouped.shift(0) - grouped.cummin()) / (1 + grouped.cummax() - grouped.cummin())\n",
    "\n",
    "    # Some items are can be inflation dependent\n",
    "    # and some items are very \"stable\"\n",
    "    df['price_nunique'] = df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "    df['item_nunique'] = df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "    \n",
    "    df['price_momentum'] = df['sell_price']/df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    df['price_momentum_m'] = df['sell_price']/df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "    df['price_momentum_y'] = df['sell_price']/df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "    \n",
    "    \n",
    "    logger.info('Prices done')\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_time_features(df, dt_col):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    attrs = [\"year\", \"quarter\", \"month\", \"week\", \"day\", \"dayofweek\", \"is_year_end\", \"is_year_start\", \"is_quarter_end\", \\\n",
    "        \"is_quarter_start\", \"is_month_end\",\"is_month_start\", 'dayofyear', 'weekofyear', 'is_month_end'\n",
    "    ]\n",
    "\n",
    "    for attr in attrs:\n",
    "        dtype = np.int16 if attr == \"year\" else np.int8\n",
    "        df[attr] = getattr(df['date'].dt, attr).astype(dtype)\n",
    "            \n",
    "    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)   \n",
    "    df['has_event'] = (df['event_name_1'] != 'None').astype(int)\n",
    "    \n",
    "    ## Adding the embedded vectors (similar to John's, trying both ways)\n",
    "    df['wd1'] =0\n",
    "    df['wd2'] =0\n",
    "    df['wd3'] =0\n",
    "    df['wd4'] =0\n",
    "\n",
    "    df.loc[:,'wd1'][df['dayofweek'] =='Sunday'] , df.loc[:,'wd2'][df['dayofweek'] =='Sunday'],\\\n",
    "    df.loc[:,'wd3'][df['dayofweek'] =='Sunday'] , df.loc[:,'wd4'][df['dayofweek'] =='Sunday']= 0.4 ,-0.3 ,0.6,0.1\n",
    "\n",
    "    df.loc[:,'wd1'][df['dayofweek'] =='Monday'] , df.loc[:,'wd2'][df['dayofweek'] =='Monday'],\\\n",
    "    df.loc[:,'wd3'][df['dayofweek'] =='Monday'] , df.loc[:,'wd4'][df['dayofweek'] =='Monday']= 0.2 ,0.2 ,0.5,-0.3\n",
    "\n",
    "    df.loc[:,'wd1'][df['dayofweek'] =='Tuesday'] ,df.loc[:,'wd2'][df['dayofweek'] =='Tuesday'],\\\n",
    "    df.loc[:,'wd3'][df['dayofweek'] =='Tuesday'] , df.loc[:,'wd4'][df['dayofweek'] =='Tuesday']= 0.1,-1.0,1.3,0.9\n",
    "\n",
    "    df.loc[:,'wd1'][df['dayofweek'] =='Wednesday'] , df.loc[:,'wd2'][df['dayofweek'] =='Wednesday'],\\\n",
    "    df.loc[:,'wd3'][df['dayofweek'] =='Wednesday'] , df.loc[:,'wd4'][df['dayofweek'] =='Wednesday']= -0.6,0.5,1.2,0.7\n",
    "\n",
    "    df.loc[:,'wd1'][df['dayofweek'] =='Thursday'] , df.loc[:,'wd2'][df['dayofweek'] =='Thursday'],\\\n",
    "    df.loc[:,'wd3'][df['dayofweek'] =='Thursday'] , df.loc[:,'wd4'][df['dayofweek'] =='Thursday']= 0.9,0.2,-0.1,0.6\n",
    "\n",
    "    df.loc[:,'wd1'][df['dayofweek'] =='Friday'] , df.loc[:,'wd2'][df['dayofweek'] =='Friday'],\\\n",
    "    df.loc[:,'wd3'][df['dayofweek'] =='Friday'] , df.loc[:,'wd4'][df['dayofweek'] =='Friday']= 0.4,1.1,0.3,-1.5\n",
    "\n",
    "    df.loc[:,'wd1'][df['dayofweek'] =='Saturday'] , df.loc[:,'wd2'][df['dayofweek'] =='Saturday'],\\\n",
    "    df.loc[:,'wd3'][df['dayofweek'] =='Saturday'] , df.loc[:,'wd4'][df['dayofweek'] =='Saturday']= 0.3,-0.2,0.6,0.0\n",
    "\n",
    "    def circle_encode(df, col):\n",
    "\n",
    "        maxval = float(df[col].max())\n",
    "\n",
    "        cosval = np.cos(2 * np.pi * df[col]/maxval)\n",
    "        sinval = np.sin(2 * np.pi * df[col]/maxval)\n",
    "\n",
    "        return cosval, sinval\n",
    "    \n",
    "    \n",
    "    df['cos_month'], df['sin_month'] = circle_encode(df, 'month')\n",
    "    df['cos_doy'], df['sin_doy'] = circle_encode(df, 'dayofyear')\n",
    "    df['cos_woy'], df['sin_woy'] = circle_encode(df, 'weekofyear')\n",
    "    \n",
    "    logger.info('Time done')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "## weather conditions, special events, \"feature discovery\"\n",
    "def add_original_features(df):\n",
    "    df['shift_t28_log']   = np.log(df['shift_t28'] + 1)\n",
    "    df['shift_t28_sqrt']  = np.sqrt(df['shift_t28'])\n",
    "    df['shift_t56_log']   = np.log(df['shift_t56'] + 1)\n",
    "    df['shift_t56_sqrt']  = np.sqrt(df['shift_t56'])\n",
    "\n",
    "    df['shift_t28_diff_t7'] = df.groupby('id')['shift_t28'].diff(7)\n",
    "    df['shift_t56_diff_t7'] = df.groupby('id')['shift_t56'].diff(7)\n",
    "    \n",
    "    logger.info('Others done')\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_demand_features(data).pipe(reduce_mem_usage)\n",
    "dt_col = \"date\"\n",
    "data = add_time_features(data, dt_col).pipe(reduce_mem_usage)\n",
    "data = data.sort_values(\"date\")\n",
    "data = add_price_features(data).pipe(reduce_mem_usage)\n",
    "data = add_original_features(data).pipe(reduce_mem_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['id', 'date', 'part']\n",
    "features = [col for col in data.columns if col not in drop_cols]\n",
    "\n",
    "is_train = (data[\"d\"] < 1914)\n",
    "is_valid = (data[\"d\"] >= 1914) & (data[\"d\"] < 1942)\n",
    "\n",
    "is_private = (data[\"d\"] >= 1942)\n",
    "is_public = ~(data[\"d\"] < 1914) & ~(is_private)\n",
    "\n",
    "day_col = ['d']\n",
    "\n",
    "# Attach \"d\" to X_train for cross validation.\n",
    "X_train = data[is_train][day_col + features].reset_index(drop=True)\n",
    "y_train = data[is_train][\"demand\"].reset_index(drop=True)\n",
    "X_valid = data[is_valid][day_col + features].reset_index(drop=True)\n",
    "y_valid = data[is_valid][\"demand\"].reset_index(drop=True)\n",
    "\n",
    "# del data\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "X_train = X_train.drop(['d', 'demand'], axis=1)\n",
    "X_valid = X_valid.drop(['d', 'demand'], axis=1)\n",
    "\n",
    "# Create template to insert predictions\n",
    "id_date_pub = data[is_public][[\"id\", \"date\"]].reset_index(drop=True)\n",
    "id_date_pri = data[is_private][[\"id\", \"date\"]].reset_index(drop=True)\n",
    "X_test_pub = data[is_public][features].reset_index(drop=True)\n",
    "X_test_pri = data[is_private][features].reset_index(drop=True)\n",
    "\n",
    "X_test_pub = X_test_pub.drop(['d', 'demand'], axis=1)\n",
    "X_test_pri = X_test_pri.drop(['d', 'demand'], axis=1)\n",
    "\n",
    "# print(\"X_train shape:\", X_train.shape)\n",
    "# print(\"X_test_pub shape:\", X_test_pub.shape)\n",
    "# print(\"X_test_pri shape:\", X_test_pri.shape)\n",
    "# print(\"id_date_pub shape:\", id_date_pub.shape)\n",
    "# print(\"id_date_pri shape:\", id_date_pri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling\n",
    "\n",
    "- Since LGBM uses GOSS (sampling histograms for tree feature splits) it is way faster than xgboost. If there is no outliers it can reach almost the same results in 1/5 the time.\n",
    "\n",
    "- Exclusive Feature Bundling - We generally work with high dimensionality data. Such data have many features which are mutually exclusive i.e they never take zero values simultaneously. LightGBM safely identifies such features and bundles them into a single feature to reduce the complexity to O(#data * #bundle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/lightgbm-parameters-guide\n",
    "\n",
    "logger.info('Creating LGB Regressor...')\n",
    "\n",
    "lgb_regressor = lgb.LGBMRegressor(silent=False)\n",
    "\n",
    "\n",
    "# gbdt: 500 iter: 2.12, 2.08\n",
    "# goss: 500 iter: 2.16, 2.09\n",
    "# dart: 500 iter: 2.24, 2.14\n",
    "\n",
    "# standard model early stops at 1000 iterations: 2.07, 2.08\n",
    "# using target variable logged didn't improve\n",
    "\n",
    "\n",
    "\n",
    "lgb_params = {\"max_depth\": [25,50, 75],\n",
    "              \"learning_rate\" : [0.01,0.05,0.1],\n",
    "              \"num_leaves\": [30,40,60],\n",
    "              \"n_estimators\": [200], \n",
    "              'objective': ['tweedie']\n",
    "             }\n",
    "\n",
    "\n",
    "logger.info('Generating grid for GSCV...')\n",
    "\n",
    "lgb_grid = GridSearchCV(lgb_regressor,\n",
    "                    lgb_params, \n",
    "                    n_jobs=8,\n",
    "                    scoring=\"neg_root_mean_squared_error\",\n",
    "                    verbose=5,\n",
    "                    cv=3)\n",
    "\n",
    "logger.info('Fitting the grid...')\n",
    "\n",
    "lgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"grid scores\")\n",
    "print (lgb_grid.grid_scores_)\n",
    "print('best params')\n",
    "print (lgb_grid.best_params_)\n",
    "print('best score')\n",
    "print (lgb_grid.best_score_)\n",
    "\n",
    "\n",
    "logger.info('HP Tuning complete!')\n",
    "\n",
    "# del X_train, y_train\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(\n",
    "        X_train,\n",
    "        label=y_train,\n",
    "        categorical_feature=[\"item_id\"]\n",
    "    )\n",
    "\n",
    "#to use early stopping\n",
    "valid_set = lgb.Dataset(\n",
    "        X_valid,\n",
    "        label=y_valid,\n",
    "        categorical_feature=[\"item_id\"]\n",
    "    )\n",
    "\n",
    "lgb_best_params =  {\n",
    "#                'lambda_l1': ...,\n",
    "#                'lambda_l2': ...,\n",
    "               'num_leaves': 20, \n",
    "#                'feature_fraction': ...,\n",
    "#                'bagging_fraction': ..., \n",
    "#                'bagging_freq': ..., \n",
    "#                'min_child_samples': ..., \n",
    "               'boosting_type': 'gbdt',   # 'dart', 'goss', 'gbdt', 'rf'\n",
    "               'metric': 'rmse',\n",
    "               'objective': 'poisson',\n",
    "               'n_jobs': -1,\n",
    "               'seed': 42,\n",
    "#                'learning_rate': ...,\n",
    "#                'min_data_in_leaf': ...\n",
    "}\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "logger.info('Training best LGBM model...')\n",
    "\n",
    "lgb_best_model = lgb.train(\n",
    "    params = lgb_best_params,\n",
    "    train_set = train_set,\n",
    "    valid_sets = [valid_set, train_set],\n",
    "    valid_names = ['eval', 'train'],\n",
    "    evals_result = evals_result,\n",
    "    #fit params\n",
    "    num_boost_round = 2000,\n",
    "    early_stopping_rounds = 50,\n",
    "    verbose_eval = 100,\n",
    ")\n",
    "logger.info('Best LGBM model train is complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importance()\n",
    "names = model.feature_name()\n",
    "fi = pd.DataFrame(importances, names)\n",
    "# fi.set_index('names')\n",
    "fi.sort_values(by=0, ascending=False).head(30).plot.barh(figsize=(5,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp_type = \"gain\"\n",
    "preds_pub = np.zeros(X_test_pub.shape[0])\n",
    "preds_pri = np.zeros(X_test_pri.shape[0])\n",
    "\n",
    "preds_pub = model.predict(X_test_pub)\n",
    "preds_pri = model.predict(X_test_pri)\n",
    "\n",
    "\n",
    "val_score = rmse(preds_pub, y_valid)\n",
    "val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=y_valid)\n",
    "D_test = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "xgb_regressor = xgb.XGBRegressor()\n",
    "xgb_params = {\n",
    "     \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "     }\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb_regressor,\n",
    "                    xgb_params, \n",
    "                    n_jobs=-1,\n",
    "                    scoring=\"rmse\",\n",
    "                    cv=3)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print (grid.grid_scores_)\n",
    "print('best params')\n",
    "print (grid.best_params_)\n",
    "print('best score')\n",
    "print (grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params =  {\n",
    "    'booster': 'gbtree',\n",
    "#     'learning_rate': ,\n",
    "#     'gamma': ,\n",
    "#     'lambda': ,\n",
    "#     'alpha': ,\n",
    "#     'max_leaves': ,\n",
    "    'learning_rate': 0.3, \n",
    "    'max_depth': 3,  \n",
    "#     'objective': ,  \n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 42    \n",
    "}\n",
    "\n",
    "logger.info(\"Creating and fitting model...\")\n",
    "\n",
    "xgb_best_model = xgb.XGBRegressor(\n",
    "                 verbosity=2\n",
    "                 colsample_bytree=0.4,\n",
    "                 gamma=0,                 \n",
    "                 learning_rate=0.07,\n",
    "                 max_depth=3,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=10000,                                                                    \n",
    "                 reg_alpha=0.75,\n",
    "                 reg_lambda=0.45,\n",
    "                 subsample=0.6,\n",
    "                 seed=42,\n",
    "                 num_round = 200,\n",
    "                 early_stopping_rounds = 50,\n",
    "                 verbose_eval = 100) \n",
    "\n",
    "xgb_best_model.fit(X_train,y_train)\n",
    "logger.info(\"Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OrderedDict(sorted(model_xgb.booster().get_fscore().items(), key=lambda t: t[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_cb = {'depth': [4, 7, 10],\n",
    "          'learning_rate' : [0.03, 0.1, 0.15],\n",
    "         'l2_leaf_reg': [1,4,9],\n",
    "         'iterations': [300]}\n",
    "regressor_cb = cb.CatBoostRegressor()\n",
    "grid_cb = GridSearchCV(regressor_cb, params_cb, scoring=\"rmse\", cv = 3)\n",
    "grid_cb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(test_pub, test_pri, submission):\n",
    "    preds_pub = test_pub[[\"id\", \"date\", \"demand\"]]\n",
    "    preds_pub['id'] = preds_pub['id'].str.replace(\"_evaluation\", \"_validation\")\n",
    "#     print(preds_pub['id'].head())\n",
    "    preds_pri = test_pri[[\"id\", \"date\", \"demand\"]]\n",
    "    # 01-28: validation\n",
    "    # 29-56: evaluation\n",
    "    val_dur  = preds_pub[\"date\"]<\"2016-05-23\"\n",
    "\n",
    "    preds_val  = preds_pub[val_dur]\n",
    "    preds_eval = preds_pri#[eval_dur]\n",
    "\n",
    "    preds_val = preds_val.pivot(index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n",
    "    preds_eval = preds_eval.pivot(index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n",
    "    \n",
    "    logger.info(f'Predictions validation: {preds_val.shape}')\n",
    "    logger.info(f'Predictions evaluation: {preds_eval.shape}')\n",
    "    \n",
    "    \n",
    "    preds_val.columns = [\"id\"] + [\"F\" + str(d+1) for d in range(28)]\n",
    "    preds_eval.columns = [\"id\"] + [\"F\" + str(d+1) for d in range(28)]\n",
    "    \n",
    "    \n",
    "    preds_val = preds_val[preds_val['id'].str.endswith(\"validation\")]\n",
    "    preds_eval = preds_eval[preds_eval['id'].str.endswith(\"evaluation\")]\n",
    "    \n",
    "    \n",
    "    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n",
    "    vals = submission[[\"id\"]].merge(preds_val, how=\"inner\", on=\"id\")\n",
    "    \n",
    "    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n",
    "    evals = submission[[\"id\"]].merge(preds_eval, how=\"inner\", on=\"id\")\n",
    "    \n",
    "    final = pd.concat([vals, evals])\n",
    "\n",
    "    \n",
    "    logger.info(\"Predictions complete.\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = make_submission(id_date_pub.assign(demand=preds_pub), id_date_pri.assign(demand=preds_pri), submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.head()\n",
    "output.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"submission_private3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
